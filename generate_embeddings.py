# -*- coding: utf-8 -*-
"""generate_embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BxCr7Vl725a9EuLGoh6LwkszGiimmdDl
"""

import pandas as pd
import numpy as np
import torch
import tqdm

from torch.utils.data import Dataset

if torch.cuda.is_available():
  device = torch.device("cuda")
else:
  device = torch.device("cpu")

print(device)

# ## Testing code

# from google.colab import drive
# drive.mount('/content/drive')

# import sys
# sys.path.append('/content/drive/My Drive/Stance Detection')
# !pip install transformers

from transformers import RobertaTokenizer, RobertaModel

tokenizer = RobertaTokenizer.from_pretrained("distilroberta-base")
model = RobertaModel.from_pretrained("distilroberta-base", output_hidden_states=True)
if torch.cuda.is_available():
  model.cuda()

class Generator(Dataset):
  def __init__(self, stances, bodies, body_id):
    self.max_len = 512
    self.stances = stances
    self.bodies = bodies
    self.articleBody = self.bodies['articleBody']
    self.headline = self.stances['Headline']
    self.headline_id = self.stances['Headline ID']
    if len(self.stances) == 0:
      return
    self.body_id = body_id
  
  def __len__(self):
    if len(self.stances) == 0:
      return 0
    return 1

  def __getitem__(self, idx):
    if len(self.stances) == 0:
      return None
    self.add_data()
    return self.body_id, self.headEmbedding, self.bodyEmbedding, self.stance, self.headline_id.to_list()

  def generate_embeddings(self, row):
    if len(row) > 2 * self.max_len:
      sentences = [0]
      sentences.extend([i + 1 for i, ch in enumerate(row) if ch == '.'])
      l = len(sentences)
      row = row[sentences[l // 2]:self.max_len]

    tokenized = tokenizer.encode(row, add_special_tokens=True)
    padded = np.array(tokenized + [0]*(self.max_len - len(tokenized)))
    attention_mask = np.where(padded != 0, 1, 0)

    tokenized = tokenized[0:self.max_len]
    padded = padded[0:self.max_len]
    attention_mask = attention_mask[0:self.max_len]

    input_ids = torch.as_tensor(padded).unsqueeze(0)
    attention_mask = torch.as_tensor(attention_mask).unsqueeze(0)

    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    with torch.no_grad():
        output = model(input_ids, attention_mask=attention_mask)
    
    embedding = torch.mean(torch.stack(output[2][4:7]), dim=0)

    return embedding

  def map_stances(self, stance):
    if stance == 'unrelated':
      return 0
    if stance == 'disagree':
      return 1
    if stance == 'agree':
      return 2
    if stance == 'discuss':
      return 3

  def add_data(self):
    embedding_list = []
    for row in self.stances['Headline']:
      embedding = self.generate_embeddings(row)
      embedding_list.append(embedding)
    self.headEmbedding = torch.cat(embedding_list, dim=0).to(device)
    for row in self.bodies['articleBody']:
      embedding_list = []
      embedding = self.generate_embeddings(row)
      embedding_list.append(embedding)
    self.bodyEmbedding = torch.cat(embedding_list, dim=0).to(device)
    self.stance = self.stances['Stance'].apply(self.map_stances).to_numpy()
    self.stance = torch.as_tensor(self.stance).to(device)

class IdxGenerator(Generator):
  def __init__(self, stances, bodies, body_id, neg_idx, dist_train=False, embedding=True):
    super().__init__(stances, bodies, body_id)
    self.neg_idx = neg_idx
    self.dist_train = dist_train
    self.embedding = embedding
    self.article_body_list = []
    self.article_headline_id_list = []

  def __len__(self):
    if len(self.stances) == 0:
      return 0
    stance = self.stances['Stance'].apply(self.map_stances).to_numpy()
    if self.embedding == False:
      return 1
    if self.dist_train and (len(stance[stance == self.neg_idx]) == 0 or len(stance[stance > self.neg_idx]) == 0):
      return 0
    if self.dist_train == False and len(stance[stance == self.neg_idx]) == 0 and len(stance[stance > self.neg_idx]) == 0:
      return 0
    if self.add_data() == False:
      return 0
    return 1

  def __getitem__(self, idx):
    if len(self.stances) == 0:
      return None
    stance = self.stances['Stance'].apply(self.map_stances).to_numpy()
    if self.embedding == False:
      return self.body_id, torch.from_numpy(stance), self.articleBody.tolist(), self.headline_id.tolist()
    if self.dist_train and (len(stance[stance == self.neg_idx]) == 0 or len(stance[stance > self.neg_idx]) == 0):
      return None
    if self.dist_train == False and len(stance[stance == self.neg_idx]) == 0 and len(stance[stance > self.neg_idx]) == 0:
      return None
    return self.body_id, self.headEmbedding, self.bodyEmbedding, self.stance, self.article_headline_id_list

  def add_data(self):
    embedding_list = []
    stance = []
    self.stance = self.stances['Stance'].apply(self.map_stances).tolist()
    idx = 0
    article_headline_id_list = []
    for row, headline_id in zip(self.stances['Headline'], self.stances['Headline ID']):
      if self.stance[idx] < self.neg_idx:
        idx += 1
        continue
      embedding = self.generate_embeddings(row)
      embedding_list.append(embedding)
      stance.append(self.stance[idx])
      article_headline_id_list.append(headline_id)
      idx += 1
    self.headEmbedding = torch.cat(embedding_list, dim=0).to(device)
    if len(embedding_list) == 0:
      return False
    embedding_list = []
    for row in self.bodies['articleBody']:
      embedding = self.generate_embeddings(row)
      embedding_list.append(embedding)
    self.article_headline_id_list = article_headline_id_list
    self.bodyEmbedding = torch.cat(embedding_list, dim=0).to(device)
    self.stance = torch.as_tensor(stance).to(device)

"""**This class is of interest from the user point of view**"""

class DatasetIterator:
  def __init__(self, PATH_STANCES, PATH_BODIES, dist_train=False, body_id=0, max_len=2533, embedding=True):
    self.body_id = body_id
    self.max_len = max_len
    self.PATH_STANCES = PATH_STANCES
    self.PATH_BODIES = PATH_BODIES
    self.dist_train = dist_train
    self.embedding = embedding

  def next(self, progress=None, neg_idx=None):
    self.stances_csv = pd.read_csv(self.PATH_STANCES)
    self.bodies_csv = pd.read_csv(self.PATH_BODIES)
    while self.body_id < self.max_len:
      self.stances = self.stances_csv[self.stances_csv['Body ID'] == self.body_id]
      self.bodies = self.bodies_csv[self.bodies_csv['Body ID'] == self.body_id]
      if neg_idx is None:
        g = Generator(self.stances, self.bodies, self.body_id)
      else:
        g = IdxGenerator(self.stances, self.bodies, self.body_id, neg_idx, self.dist_train, self.embedding)
      self.body_id += 1
      if progress is not None:
        progress.update(1)
      if len(g) == 0:
        continue
      return torch.utils.data.DataLoader(g)
    return None

# ## Testing Code
# import tqdm

# PATH_TRAIN_STANCES = '/content/drive/My Drive/Stance Detection/dataset/train_stances.csv'
# PATH_TRAIN_BODIES = '/content/drive/My Drive/Stance Detection/dataset/train_bodies.csv'

# d = DatasetIterator(PATH_STANCES=PATH_TRAIN_STANCES, PATH_BODIES=PATH_TRAIN_BODIES, max_len=100)

# epochs = 1
# progress = tqdm.tqdm(total=d.max_len*epochs, position=0, leave=True)

# while True:
#   dataloader = d.next()
#   if dataloader is None:
#     break
#   for body_id, headEmbedding, bodyEmbedding, stance, headline in dataloader:
#     print(body_id, headEmbedding.shape, bodyEmbedding.shape, stance.shape, len(headline))